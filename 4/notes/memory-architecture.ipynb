{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Memory heirarchy** is organization of memory to reduce latency of data transfer between the main memory of a computer and a CPU. At the top of the heirarchy we find registers since they're inside the CPU and therefore need the least amount of time to access, then we have cache memory which is usually implemented using Static RAM (SRAM); Main Memory is next down on the heirarchy and is usally implemented using Dynamic RAM (DRAM – probably because this stuff gets written to a lot)f; Solid State Drives or Secondary Storage follow and are also the bottom of the heirarchy. The heirarchy acts as a model for us to gage the capacity, access time, performance, and cost per bit for each type of memory. Capcity is straightforward and is just how much volume of memory the heirarchy level can store and increases as you go down the heirarchy. Access time is the time it takes for read/write requests to make it to that memory and decreases as you go down the heirarchy. Performance is how optimal your \"split\" is e.g. having a lot of Secondary Storage might give you a lot of capcity by slow access time degrading your computer's memory performance – it's kinda like the balance you have to strike in order to get the best of all worlds; the cost of a bit increases as you go up the heirarchy **(4.1)**. *Caches* are broken up into a few pieces (L1-L3) where the number corresponds to the latency and size of the cache (just conceptually) i.e. L1 is the fastest to access but has the least amount of data. The reason DRAM is called what it is is because it uses capacitors to store its memory and those capacitors need to be refreshed regularaly. SRAM on the other hand doesn't need to be refreshed which makes it much faster but also more expesnive. The cache is there as a middle man between DRAM and the CPU to speed up data by holding important or relevant info it thinks the CPU will need to access many times. L1 is on the actual CPU itself while L2 is outside of it and *catches any recent data accesses that L1 didn't catch*. L3 does the same thing as L2 does for L1 but instead for L2. The CPU searches the caches from L1-L3 for data it needs and then moves on to DRAM if it can't find it **(4.2)**. Cache is *associative* memory which means you can use more than just the address of memory to store it in there i.e. you can also use a tag. The tag stores the memory address of the data while the rest of the cache holds the data. The cache also stores a **valid bit** for every instruction so that the processor doesn't confuse garbage data for an actual instruction (video 1). The CPU can *flush* data out of the cache to invalidate the memory in there; this is useful when the CPU loads new memory from SSD. This video reallly goes in DEPTH about how the CPU goes about validating memory access!! Basically the tag field holds the address of stuff in the cache and the CPU can use it to validate whether the memory it's trying to access is what it wants e.g. if the tag address has a value of 40 but the CPU is looking for a tag with a value of 58, that request will result in a *Cache Miss*, otherwise, if they match up, it results in a *Cache Hit* (video 2). If you want to make better use of your cache, you can increase the *block size* or the amount of memory used to store the actual data and not the address i.e. your tag is made up of 27 bits including the valid bit but your block size might only store a word of data so you have a huge overhead with the tag and not so much space to store data; you can however increase the size of your block so that the tag overhead is less! This has a limit of course and the best Average Memory Access Time (AMAT) today is about 64 bytes or 16 words. *Caches Lines* cannot be used by multiple instructions at the same time which can make direct map caching not that great; this is known as an *address conflict*. I think this direct cache mapping is only using one line to access memory?? (look at the video for more details - video 3). Ah, so I think each cache is connected to the CPU via 1 line / wire but each cache can hold more than just one piece of information therefore you can have conflicts if one line asks for memory in a cache that lines up with several tags **(Review 4.3)**. Sometimes you need more actual memory though which is where *virtual memory* comes in. This is kind of like swapping in linux where when you're RAM is running low, you can *page a file* to Secondary Storage so that you're RAM is temporarily freed up. By the way, main memory consists of both RAM and virtual memory! *Pages* are what you call blocks in virtual memory while *frames* are what blocks are called in RAM. Due to the discrepancy between RAM and SSD the only thing used to translate RAM addresses to SSD is a tag and an offset. The tag identifies the page or frame in RAM or SSD while the offset identifies the location within that frame or page to copy / move. \n",
    "\n",
    "OK lets recap real quick; I'm using [this link](https://csillustrated.berkeley.edu/PDFs/handouts/cache-3-associativity-handout.pdf) as a reference. **Directly Mapped Caches** have an index for each block or a line for each block which makes them strict in the placement of data / blocks. **Associative Caches** are kind of just one big memory space where you can fit as many blocks as you want and just have to use an offset to find your block of memory. I still don't get it. \n",
    "\n",
    "*Logical Addresses* are just offsets paired with a page number i.e. you go to the second page or frame and then offset yourself by an amount to get the data you want. **Your offset will have as many bits as it needs to address your entire cache** so if you have a 32bit architecture and your cache holds 4Kb of memory then you need 12 bits (2^12) to contain all the space in your cache – this is only true for your fully associative caches since they have no sets. Each process is assigned its own page and can be looked up through a Page Table Base Register which is like an array of the indexes of pages you have. Controlling virtual memory comes with a few bits, one bit to validate read access, one write access, and another execute access or the ability to fetch instructions from an address space (the OS writes the bits and the processor only reads them). Replacement bits allow for the OS to write to pages and there are two bits: the \"dirty\" bit allows for writing to a page while the reference bit allows for reading (external textbook). The CPU needs to generate virtual addresses (VAs) and needs a way to distinguish those from physical address (PAs). A Memory Mangement Unit (MMU) is used to translate Vas to PAs. The MMU uses a Page Table to look up PAs from VAs; the page table is a key-value pair table using the VA to lookup / index PAs. PAs in the Page Table that don't have a correspondent VA can be seen as PAs that have not yet had a VA space allocated for them (4.4). While Assembly has access to registers on the CPU and PAs and VAs using logical addresses, the cache is largely managed by the OS. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
