{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Memory heirarchy** is organization of memory to reduce latency of data transfer between the main memory of a computer and a CPU. At the top of the heirarchy we find registers since they're inside the CPU and therefore need the least amount of time to access, then we have cache memory which is usually implemented using Static RAM (SRAM); Main Memory is next down on the heirarchy and is usally implemented using Dynamic RAM (DRAM – probably because this stuff gets written to a lot)f; Solid State Drives or Secondary Storage follow and are also the bottom of the heirarchy. The heirarchy acts as a model for us to gage the capacity, access time, performance, and cost per bit for each type of memory. Capcity is straightforward and is just how much volume of memory the heirarchy level can store and increases as you go down the heirarchy. Access time is the time it takes for read/write requests to make it to that memory and decreases as you go down the heirarchy. Performance is how optimal your \"split\" is e.g. having a lot of Secondary Storage might give you a lot of capcity by slow access time degrading your computer's memory performance – it's kinda like the balance you have to strike in order to get the best of all worlds; the cost of a bit increases as you go up the heirarchy **(4.1)**. *Caches* are broken up into a few pieces (L1-L3) where the number corresponds to the latency and size of the cache (just conceptually) i.e. L1 is the fastest to access but has the least amount of data. The reason DRAM is called what it is is because it uses capacitors to store its memory and those capacitors need to be refreshed regularaly. SRAM on the other hand doesn't need to be refreshed which makes it much faster but also more expesnive. The cache is there as a middle man between DRAM and the CPU to speed up data by holding important or relevant info it thinks the CPU will need to access many times. L1 is on the actual CPU itself while L2 is outside of it and *catches any recent data accesses that L1 didn't catch*. L3 does the same thing as L2 does for L1 but instead for L2. The CPU searches the caches from L1-L3 for data it needs and then moves on to DRAM if it can't find it **(4.2)**. Cache is *associative* memory which means you can use more than just the address of memory to store it in there i.e. you can also use a tag. The tag stores the memory address of the data while the rest of the cache holds the data. The cache also stores a **valid bit** for every instruction so that the processor doesn't confuse garbage data for an actual instruction (video 1). The CPU can *flush* data out of the cache to invalidate the memory in there; this is useful when the CPU loads new memory from SSD. This video reallly goes in DEPTH about how the CPU goes about validating memory access!! Basically the tag field holds the address of stuff in the cache and the CPU can use it to validate whether the memory it's trying to access is what it wants e.g. if the tag address has a value of 40 but the CPU is looking for a tag with a value of 58, that request will result in a *Cache Miss*, otherwise, if they match up, it results in a *Cache Hit* (video 2). If you want to make better use of your cache, you can increase the *block size* or the amount of memory used to store the actual data and not the address i.e. your tag is made up of 27 bits including the valid bit but your block size might only store a word of data so you have a huge overhead with the tag and not so much space to store data; you can however increase the size of your block so that the tag overhead is less! This has a limit of course and the best Average Memory Access Time (AMAT) today is about 64 bytes or 16 words. *Caches Lines* cannot be used by multiple instructions at the same time which can make direct map caching not that great; this is known as an *address conflict*. I think this direct cache mapping is only using one line to access memory?? (look at the video for more details - video 3). Ah, so I think each cache is connected to the CPU via 1 line / wire but each cache can hold more than just one piece of information therefore you can have conflicts if one line asks for memory in a cache that lines up with several tags **(Review 4.3)**. Sometimes you need more actual memory though which is where *virtual memory* comes in. This is kind of like swapping in linux where when you're RAM is running low, you can *page a file* to Secondary Storage so that you're RAM is temporarily freed up. By the way, main memory consists of both RAM and virtual memory! *Pages* are what you call blocks in virtual memory while *frames* are what blocks are called in RAM. Due to the discrepancy between RAM and SSD the only thing used to translate RAM addresses to SSD is a tag and an offset. The tag identifies the page or frame in RAM or SSD while the offset identifies the location within that frame or page to copy / move. \n",
    "\n",
    "OK lets recap real quick; I'm using [this link](https://csillustrated.berkeley.edu/PDFs/handouts/cache-3-associativity-handout.pdf) as a reference. **Directly Mapped Caches** have an index for each block or a line for each block which makes them strict in the placement of data / blocks. **Associative Caches** are kind of just one big memory space where you can fit as many blocks as you want and just have to use an offset to find your block of memory. I still don't get it. \n",
    "\n",
    "*Logical Addresses* are just offsets paired with a page number i.e. you go to the second page or frame and then offset yourself by an amount to get the data you want. **Your offset will have as many bits as it needs to address your entire cache** so if you have a 32bit architecture and your cache holds 4Kb of memory then you need 12 bits (2^12) to contain all the space in your cache – this is only true for your fully associative caches since they have no sets. Each process is assigned its own page and can be looked up through a Page Table Base Register which is like an array of the indexes of pages you have. Controlling virtual memory comes with a few bits, one bit to validate read access, one write access, and another execute access or the ability to fetch instructions from an address space (the OS writes the bits and the processor only reads them). Replacement bits allow for the OS to write to pages and there are two bits: the \"dirty\" bit allows for writing to a page while the reference bit allows for reading (external textbook). The CPU needs to generate virtual addresses (VAs) and needs a way to distinguish those from physical address (PAs). A Memory Mangement Unit (MMU) is used to translate Vas to PAs. The MMU uses a Page Table to look up PAs from VAs; the page table is a key-value pair table using the VA to lookup / index PAs. PAs in the Page Table that don't have a correspondent VA can be seen as PAs that have not yet had a VA space allocated for them (4.4). While Assembly has access to registers on the CPU and PAs and VAs using logical addresses, the cache is largely managed by the OS (4.5). Memory is *segmented* into logical groupings or ranges of bytes. Segments can have differing uses / purposes. As with Assembly, we use segments too such as the the .data directive / segment meant for storing data and the .text segment meant for storing instructions. The heap is used by the data segment, and the stack is used for temp data in programs **(4.7)**. The reason for not having dynamic data directives in Assembly is for one, we use registers to hold pointers whereas in higher level languages, RAM locations hold the pointer to *another* RAM location. Stack data also has some special purpose (sp) hardware for it so we don't really need a directive for it **(4.8)**. Assembly doesn't usually allow for direct access to the different layers of memory architecture besides registers. We mostly just let the underlying computer architecture control where in virtual memory and cache our instructions will go **(4.9)**. There are some registers purpose meant for manipulating the stack memory (known amounts of memory unlike linked lists which go onto the heap) and one register that is a pointer to the stack is approprately named ^sp. Stack memory is appropriately named from the stack data structure i.e. peeking at the stack returns you the last added item and removing an item removes the top most item. ^sp is an alias for ^29 in MIPS and will always point to the top of the stack. **The stack is UPSIDE DOWN so if you want to ADD / PUSH something to it, in reality, you actually have to SUBTRACT by the amount of data to get the NEXT element!!! To POP something from the stack you have to ADD the data amount to the stack pointer!!** Will the actual piece of memory get deallocated or has it just been copied elsewhere and still exists in that memory address? How does one actually deallocate something in Assembly?? The stack is usually used for temp data which means you'll probably only have to use it if your registers are all in use **(4.10)**. The reason the stack is portrayed upside down is because it shares the same memory segment as the dynamic data segment and as a program runs, the dynamic data memory grows upward while the stack memory grows downward where they can meet resulting in no memory for your computers [virtual memory!!]. For example, your dynamic data / stack memory segment can have a range of 1000 0000 -- 7FFF FFFF where the stack will start at 7FFF FFF8 (so that it doens't intrude on the next segment) and the DD memory will start at 1000 0008 **(4.11)**. Functions or *procedures* are a way of calling code from somewhere else instead of having to write the whole text over and over again. When calling the function there is the *caller* and the *callee*. The callee can be called from anywhere using its address but when it returns its value it'll have remember where the different places it was called from in order to reutrn to the right place in the program to continue on with the rest of it. To do this MIPS uses two instructions, one being **jal** (jump and link) where it links the procedure you want to call and **puts the address of the next instruciton into a special register called the return address register (^ra). At the end of the procedure, jr ^ra (jump return) is called so your program is now looking at the instruction that was after the caller**. Your procedures can *shadow* or overwrite registers you're using for storage so you have to be careful of what register is doing what!! | In essence, you need to know where to expect data (what registers are meant to be used as arguments and which ones are meant to be used as results). Sometimes registers will overwrite each other which is why the notion of saving and restoring is a good one to have; saving data to the stack \"protects\" it from being lost and can be easily written to or copied from. Each procedure will manage its own stack space so it doesn't interfere with callers or callees uses of registers. Remember that to add to a stack you first have to *decrement* the stack pointer and *then* store the word you want to save:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "procedure:\n",
    "    # ADD to the stack\n",
    "    subi $sp, $sp, 4\n",
    "    sw $t0, $sp\n",
    "\n",
    "    # SUBTRACT from the stack\n",
    "    lw $t0, $sp\n",
    "    addi $sp, $sp, 4\n",
    "\n",
    "# example of saving and restoring registers\n",
    "\n",
    "procedure:\n",
    "    subi $sp, $sp, 8 # move stack pointer down two words\n",
    "    sw $t0, 4($sp)\n",
    "    sw $t1, 0($sp)\n",
    "    ... # overwrite $t0 and $t1\n",
    "    lw $t1, 0($sp)\n",
    "    lw $t0, 4($sp)\n",
    "    addi $sp, $sp, 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *t* registers (^t-x) are meant for storing temp data which is what your callers would save, but there are also *s* registers (^s-x) meant for saving data throughout the program which the called procedures will have to write to / save. Put arguments into the ^a registers and save values you want callers to use in the ^v registers. To recap: *the caller must save and restore ^s-x registers while the callee must save and restore ^t-x registers* **(4.14)**. *jalr* (jump and link register) is similar to *jr* but uses a register that you specify for the address to jump to. Code examples with arguments and some other stuff **(4.15)**. When it comes to exceptions, you can look for internal problems such as errors from opeining files or external problems such as some I/O device malfunctioning. When your code detects an exception the instructions get handed off to the EPC (exception program counter) to handle the current error (interrupts are a type of exception caused by external or asynchronous problems). You can either have a general code block to deal with exceptions that looks at the *status / cause* register and then decides what to do dependent on the exception OR you can have *vectored interrupts* which calls a specific code block dependent on the error. The only real difference between these is that the first one figures out what the exception is and then runs the appropriate code block whereas the second one requires the main procedure / caller to figure out the exception and then call the correct exception handler. Page A-37 has some exception handling code if you want to look **(4.17)**. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
