{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overflow**: this happens when the number of bits on the CPU are not enough to represent a value because it is too big.\n",
    "\n",
    "**Underflow**: is pretty much the same thing as overflow but with numbers that are too small to be represented with the amount of bits the CPU has. (Is it that the CPU has or the architecture? Are those the same?)\n",
    "\n",
    "Going back to the formula (-1)^S * F * 2^E, if either the fraction value or the exponent value is too big or small to be represented with the amount of bits allocated (23 and 8 repsectively) then we can get over and underflows. \n",
    "\n",
    "**Bias**: is the idea of adding some number N to bit representation to make the lowest number zero and the highest number the highest number + N. So, in practical terms; when we have underflow circumstances, instead of the CPU freaking out and not being able to represent the value we just make it 0 so it doesn't go haywire; when we have a chance at overflow we add bias to it. I don't understand why we do this, is it because the CPU will now think the floating point number is an int or something?\n",
    "\n",
    "<h3>Single and Double Precision</h3>\n",
    "\n",
    "Single floating point preciscion uses 1 word to represent a number while double precision uses 2 words. This allows you to calculate more accurate numbers and I'm pretty sure this is where the double data type comes from. *Doubles* allow you to put on an extra 32 bits onto the exponent allocation of bits whcih means you now have more bits to represent the exponent with allowing for a more precisce number. \n",
    "\n",
    "If we throw all the jazz with bias and double point precision together, we get how MIPS actually computes floating points numbers: (-1)^S * (1 + F) * 2^(Exponent - Bias).\n",
    "\n",
    "Idk if I'm remembering correctly but I'm pretty sure F is some number between 0 and 1. Ok, it's a value GENERALLY between 0 and 1. \n",
    "\n",
    "But you might ask, what value should we use for the bias? Of course there's an international organization for this stuff (IEEE) who said \"yuh, bias be 128 for single precision, and uhh, make it 1023 for double.\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
